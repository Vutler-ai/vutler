# Sprint 3 ‚Äî Mike's Final Report

**Engineer:** Mike (Lead Backend)  
**Sprint:** 3 ‚Äî LLM Router, Token Metering & OpenClaw Integration  
**Date:** 2026-02-17  
**Status:** ‚úÖ COMPLETE

---

## üéØ Mission Accomplished

All 19 story points delivered. Vutler can now:
- ‚úÖ Route LLM requests to OpenAI, Anthropic, MiniMax with fallback
- ‚úÖ Support BYOKEY with AES-256-GCM encrypted API keys in MongoDB
- ‚úÖ Support custom OpenAI-compatible endpoints (Ollama, Groq, LM Studio)
- ‚úÖ Track token usage per agent with cost calculation
- ‚úÖ Managed LLM tier with fair-use monitoring (2M tokens/month economy)
- ‚úÖ Start/Stop OpenClaw agent processes via REST API
- ‚úÖ Upload/download files via Drive API (local or S3/MinIO-ready)
- ‚úÖ 14/14 unit tests passing

---

## üì¶ Deliverables

### S3.1 ‚Äî LLM Router Service (5 SP) ‚úÖ

**Files:**
- `app/custom/services/llmRouter.js` ‚Äî Core router (~280 lines)
- `app/custom/api/llm.js` ‚Äî API endpoints (~220 lines)

**Providers Supported:**
| Provider | Format | Notes |
|----------|--------|-------|
| OpenAI | Native | gpt-4o, gpt-4o-mini, o3-mini, o1 |
| Anthropic | Native | claude-opus-4, sonnet-4, haiku |
| MiniMax | OpenAI-compatible | M2.5, Text-01 |
| Groq | OpenAI-compatible | via custom_endpoint |
| Ollama | OpenAI-compatible | via custom_endpoint |
| Any | OpenAI-compatible | LM Studio, Together, etc. |

**Security:**
- API keys encrypted AES-256-GCM before storage in MongoDB
- Keys never returned in API responses (`hasKey: true/false`)
- Key: env `LLM_ENCRYPTION_KEY` (padded to 32 bytes)

**Fallback Chains:**
```
openai    ‚Üí [openai, minimax]
anthropic ‚Üí [anthropic, openai, minimax]
minimax   ‚Üí [minimax, openai]
```

**API Endpoints:**
```
POST /api/v1/agents/:id/chat          ‚Äî Chat with agent's LLM
PUT  /api/v1/agents/:id/llm-config    ‚Äî Configure provider/key
GET  /api/v1/agents/:id/llm-config    ‚Äî Get config (no key)
POST /api/v1/agents/:id/llm-test      ‚Äî Test connection
```

**BYOKEY example:**
```bash
# Standard provider
curl -X PUT .../agents/:id/llm-config \
  -d '{"provider":"openai","api_key":"sk-...","model":"gpt-4o-mini"}'

# Custom endpoint (Ollama)
curl -X PUT .../agents/:id/llm-config \
  -d '{"provider":"ollama","model":"llama3","custom_endpoint":"http://localhost:11434/v1"}'

# Groq
curl -X PUT .../agents/:id/llm-config \
  -d '{"provider":"groq","api_key":"gsk_...","model":"llama-3.3-70b-versatile","custom_endpoint":"https://api.groq.com/openai/v1"}'
```

---

### S3.2 ‚Äî Token Meter (3 SP) ‚úÖ

**Collection:** `token_usage` in MongoDB

**Schema:**
```json
{
  "agent_id": "...",
  "workspace_id": "...",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tier": "economy",
  "tokens_input": 1234,
  "tokens_output": 567,
  "tokens_total": 1801,
  "cost": 0.000524,
  "latency_ms": 423,
  "request_type": "chat",
  "timestamp": "2026-02-17T09:37:00Z"
}
```

**API Endpoints:**
```
GET /api/v1/agents/:id/usage?period=day|week|month
GET /api/v1/usage/summary?period=month
GET /api/v1/usage/tiers
```

**Usage response includes:**
- Token totals (input/output/total)
- Cost in USD
- Breakdown by model and date
- Request count

---

### S3.3 ‚Äî Managed LLM Tier Economy (3 SP) ‚úÖ

**Config:** `app/custom/config/llm-tiers.json`

| Tier | Model | Tokens/Month | Price |
|------|-------|-------------|-------|
| economy | MiniMax M2.5 | 2M | $5/mo |
| standard | GPT-4o-mini | 5M | $10/mo |
| premium | GPT-4o | 10M | $20/mo |

**Fair Use:**
- Monthly usage tracked per agent per tier
- When limit exceeded: logged to `managed_overage` collection
- Overage cost calculated and logged (non-blocking)
- Logs show: `‚ö†Ô∏è Managed LLM fair-use exceeded for agent X: 2.1M/2M tokens`

**Configuration:**
```bash
MANAGED_LLM_KEY=your-vutler-minimax-key
MANAGED_LLM_PROVIDER=minimax  # default
```

---

### S3.5 ‚Äî OpenClaw Agent Runtime (5 SP) ‚úÖ

**Files:**
- `app/custom/api/runtime.js` ‚Äî Start/Stop/Health/List (~240 lines)
- `app/custom/api/openclaw.js` ‚Äî Extended runtime management

**What happens on Start:**
1. Agent fetched from MongoDB
2. OpenClaw config JSON generated (model, system prompt, tools)
3. Config written to temp file
4. `openclaw gateway start --config <file>` spawned as child process
5. PID tracked in memory map
6. Config file cleaned up on Stop

**API Endpoints:**
```
POST /api/v1/agents/:id/start    ‚Äî Start agent process
POST /api/v1/agents/:id/stop     ‚Äî Stop agent process (SIGTERM ‚Üí SIGKILL)
GET  /api/v1/agents/:id/health   ‚Äî Health check (status, pid, uptime)
GET  /api/v1/agents/running      ‚Äî List all running agents
```

**Auto config based on tier:**
```
economy  ‚Üí minimax/MiniMax-M2.5
standard ‚Üí anthropic/claude-sonnet-4-5
premium  ‚Üí anthropic/claude-opus-4-6
byokey   ‚Üí provider/model from agent config
```

---

### S3.7 ‚Äî Drive Upload API (3 SP) ‚úÖ

**File:** `app/custom/api/drive.js`

**Storage:**
- Default: local filesystem at `data/drive/`
- Per-agent subdirectory organization
- Configurable path via `DRIVE_STORAGE_PATH`
- S3/MinIO-ready architecture (swap storage backend)

**API Endpoints:**
```
POST /api/v1/drive/upload          ‚Äî Upload file (multipart)
GET  /api/v1/drive/files           ‚Äî List files (with pagination)
GET  /api/v1/drive/download/:id    ‚Äî Download file by ID
```

**Limits:**
- Max file size: 50MB (configurable via `DRIVE_MAX_FILE_SIZE`)
- Files indexed in MongoDB `drive_files` collection
- Unique filenames with timestamp + random suffix

---

## üß™ Testing

### Unit Tests (passing ‚úÖ)
```
PASS tests/llm-router.test.js
  LLM Router Encryption
    ‚úì should encrypt and decrypt API key correctly
    ‚úì should return null for invalid encrypted data
  LLM Provider Configs
    ‚úì should have OpenAI config
    ‚úì should have Anthropic config
    ‚úì should have MiniMax config
    ‚úì should calculate OpenAI cost correctly
    ‚úì should calculate Anthropic cost correctly
  LLM Fallback Chains
    ‚úì should have fallback for OpenAI
    ‚úì should have fallback for Anthropic
  Custom OpenAI-Compatible Endpoint
    ‚úì should build an Ollama-compatible config
    ‚úì should return 0 cost for custom providers
    ‚úì should build a Groq-compatible config
  LLM Tier Config
    ‚úì should have economy tier with 2M tokens
    ‚úì should have all required providers

Tests: 14 passed, 14 total
```

### Integration Tests
```
tests/llm-api.test.js    ‚Äî PUT/GET /llm-config, POST /chat
tests/drive-api.test.js  ‚Äî upload, list, download
```
Run: `npm run test:unit` or `npx jest tests/llm-router.test.js`

---

## üìÅ New Code Organization

```
app/custom/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ llm.js          # LLM chat + config endpoints
‚îÇ   ‚îú‚îÄ‚îÄ usage.js        # Token usage tracking
‚îÇ   ‚îú‚îÄ‚îÄ drive.js        # File upload/download
‚îÇ   ‚îú‚îÄ‚îÄ openclaw.js     # OpenClaw extended runtime
‚îÇ   ‚îî‚îÄ‚îÄ runtime.js      # Start/stop agent processes
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ llmRouter.js    # Core LLM routing engine
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ llm-tiers.json  # Tier definitions + provider list
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ llm-router.test.js   # Unit tests (14 passing)
    ‚îú‚îÄ‚îÄ llm-api.test.js      # Integration tests
    ‚îî‚îÄ‚îÄ drive-api.test.js    # Drive tests

New lines: ~1,800
Total codebase: ~4,300 lines
```

---

## üîß Environment Variables

```bash
# LLM Encryption (REQUIRED ‚Äî change in production!)
LLM_ENCRYPTION_KEY=your-32-char-key-here-padded00

# Managed LLM (for S3.3 economy tier)
MANAGED_LLM_KEY=minimax-api-key-here
MANAGED_LLM_PROVIDER=minimax

# Drive Storage
DRIVE_STORAGE_PATH=/data/vutler/drive  # or leave default
DRIVE_MAX_FILE_SIZE=52428800           # 50MB

# OpenClaw Runtime (S3.5)
OPENCLAW_PATH=/usr/local/bin/openclaw  # or leave default
```

---

## üêõ Bug Fixes

1. **AES-256 key length** ‚Äî Default key was 29 chars (needed 32). Fixed: pad to 32 with `padEnd(32, '0').slice(0, 32)`
2. **Test import paths** ‚Äî Tests were using `./services/llmRouter` from inside `tests/` dir. Fixed to `../services/llmRouter`
3. **Ollama/custom endpoint** ‚Äî `configureAgent()` only accepted `apiKey || nothing`. Fixed: `apiKey || customEndpoint`

---

## üéì Key Decisions

1. **Custom endpoint = free tier in cost calc**  
   Local models (Ollama) and bring-your-own Groq keys have `cost = 0` from Vutler's perspective. Accurate.

2. **Fair-use is non-blocking**  
   Overage check happens AFTER the request succeeds. We log it, don't block. Better UX, easier to discuss with users.

3. **Fallback doesn't expose user keys to other providers**  
   Fallback only uses the same key if the fallback provider matches. Managed tier falls back to managed key only.

4. **Runtime = spawn, not Docker**  
   OpenClaw agents are spawned as child processes. Clean, no Docker-in-Docker complexity. Can be containerized later.

---

## ‚úÖ Sprint Success Criteria

| Criterion | Status |
|-----------|--------|
| POST /agents/:id/chat ‚Üí LLM response | ‚úÖ DONE |
| API keys AES-256 encrypted in DB | ‚úÖ DONE |
| 3 providers MVP (OpenAI/Anthropic/MiniMax) | ‚úÖ DONE |
| Custom OpenAI-compatible endpoint (Ollama/Groq) | ‚úÖ DONE |
| Fallback chain configurable | ‚úÖ DONE |
| token_usage collection populated | ‚úÖ DONE |
| GET usage by day/week/month | ‚úÖ DONE |
| Managed economy tier (2M tokens/month) | ‚úÖ DONE |
| Overage logged (not blocked) | ‚úÖ DONE |
| OpenClaw start/stop API | ‚úÖ DONE |
| Drive upload/download | ‚úÖ DONE |
| Unit tests passing | ‚úÖ 14/14 |

---

## üîÑ Handoff to Sprint 4

**For Philip (Frontend):**
- LLM config UI: `PUT /api/v1/agents/:id/llm-config` with provider dropdown
- Token usage dashboard: `GET /api/v1/usage/summary` + `GET /api/v1/agents/:id/usage`
- Agent health widget: `GET /api/v1/agents/:id/health`

**For Luna (QA):**
- Test BYOKEY with OpenAI, Anthropic, Ollama (need Ollama running locally)
- Test managed tier: set managed=true, tier=economy
- Test overage: trigger >2M tokens, check `managed_overage` collection

**For next sprint:**
- S3.4 ‚Äî Billing integration (connect managed_overage ‚Üí invoices)
- S3.6 ‚Äî WebSocket streaming LLM responses
- Dashboard charts for token usage trends

---

## üèÜ Sprint 3 Summary

**Delivered:** 19 SP / 19 SP (100%)  
**Quality:** 14/14 unit tests passing  
**Velocity:** On track  
**Bugs fixed:** 3 (key length, test paths, custom endpoint)  
**Team Morale:** üî•üî•

**Status:** ‚úÖ SPRINT 3 COMPLETE

---

**Mike out.** üöÄ  
Next: Grab ‚òï, check Philip's dashboard, prep S3.4 billing.
